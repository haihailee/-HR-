# 部署和使用指南

## 目录
- [快速开始](#快速开始)
- [详细配置](#详细配置)
- [部署到云端](#部署到云端)
- [自动化运行](#自动化运行)
- [常见问题](#常见问题)

---

## 快速开始

### 第一步：安装Python环境

确保你的电脑上已安装Python 3.8或更高版本。

检查Python版本：
```bash
python --version
```

如未安装，请访问 https://www.python.org/ 下载安装。

### 第二步：安装依赖包

在项目目录中打开终端，运行：

```bash
cd 汽车行业HR情报监控系统
pip install -r requirements.txt
```

### 第三步：配置AI服务

1. 访问智谱AI官网：https://open.bigmodel.cn/
2. 注册并登录账号
3. 在控制台创建API Key（免费用户每天有100万tokens额度）
4. 打开 `配置文件.yaml`，将API Key填入：

```yaml
ai_service:
  zhipu:
    api_key: "your_zhipu_api_key_here"  # 替换为你的API Key
    enabled: true
```

### 第四步：测试运行

#### 1. 抓取新闻数据
```bash
python 数据抓取/新闻爬虫.py
```

第一次运行可能会创建 `数据/新闻数据.json` 文件。

#### 2. AI分析新闻
```bash
python AI分析/内容分类.py
```

#### 3. 启动Web界面
```bash
streamlit run 主应用.py
```

浏览器会自动打开 http://localhost:8501，你就能看到界面了！

---

## 详细配置

### 配置文件说明（配置文件.yaml）

#### 1. AI服务配置

```yaml
ai_service:
  zhipu:
    api_key: "your_api_key"  # 必填：你的API密钥
    model: "glm-4-flash"     # 推荐使用免费的flash模型
    enabled: true             # 是否启用
```

**获取API Key的步骤：**
1. 注册智谱AI：https://open.bigmodel.cn/
2. 实名认证（需要）
3. 进入"API管理" → "创建API Key"
4. 复制Key并保存（只显示一次）

**免费额度说明：**
- glm-4-flash模型：每天100万tokens免费
- 足够分析500-1000条新闻

#### 2. 监控公司配置

```yaml
companies:
  - name: "特斯拉"
    keywords: ["特斯拉", "Tesla", "马斯克"]
    enabled: true
```

**自定义添加公司：**
- 复制一个公司配置块
- 修改 `name` 和 `keywords`
- 设置 `enabled: true` 启用

示例添加理想汽车：
```yaml
  - name: "理想汽车"
    keywords: ["理想汽车", "理想ONE", "李想"]
    enabled: true
```

#### 3. 爬虫配置

```yaml
crawler:
  request_delay: 2           # 请求间隔（秒），避免被封
  max_news_per_source: 20    # 每个来源最多抓取数量
  days_to_fetch: 7           # 只抓取最近N天的新闻
  timeout: 10                # 请求超时时间
```

**重要提示：**
- `request_delay` 不要设置太小，避免被网站封禁
- `days_to_fetch` 建议7-14天，太长会抓取过多历史数据

#### 4. 数据存储配置

```yaml
storage:
  type: "json"  # 推荐使用json，简单可靠
  json_path: "数据/新闻数据.json"
```

---

## 部署到云端（Streamlit Cloud）

### 为什么部署到云端？
- ✅ 团队成员随时随地访问
- ✅ 不需要本地电脑一直开着
- ✅ 完全免费
- ✅ 自动https和域名

### 部署步骤

#### 1. 将项目上传到GitHub

1. 在GitHub上创建新仓库（公开或私有都可以）
2. 将项目文件上传：

```bash
cd 汽车行业HR情报监控系统
git init
git add .
git commit -m "初始提交"
git branch -M main
git remote add origin https://github.com/你的用户名/仓库名.git
git push -u origin main
```

**注意：上传前务必删除配置文件中的API Key！**

在 `.gitignore` 中添加：
```
配置文件.yaml
数据/
*.json
```

#### 2. 创建Streamlit Cloud账号

1. 访问：https://streamlit.io/cloud
2. 使用GitHub账号登录
3. 点击 "New app"

#### 3. 配置部署

填写以下信息：
- **Repository**: 选择你的GitHub仓库
- **Branch**: main
- **Main file path**: 主应用.py

点击 "Advanced settings"，添加环境变量：
- 将API Key等敏感信息配置为环境变量

#### 4. 部署

点击 "Deploy"，等待2-3分钟。

部署成功后，你会得到一个公开的URL，例如：
`https://your-app.streamlit.app`

### 在Streamlit Cloud上配置API Key

修改 `主应用.py`，添加环境变量支持：

```python
import os

# 在加载配置函数中添加
def 加载配置():
    with open('配置文件.yaml', 'r', encoding='utf-8') as f:
        配置 = yaml.safe_load(f)

    # 从环境变量读取API Key（云端部署）
    if os.getenv('ZHIPU_API_KEY'):
        配置['ai_service']['zhipu']['api_key'] = os.getenv('ZHIPU_API_KEY')

    return 配置
```

在Streamlit Cloud的Settings中添加：
- Key: `ZHIPU_API_KEY`
- Value: 你的API密钥

---

## 自动化运行

### 方案一：GitHub Actions（推荐）

在项目中创建 `.github/workflows/daily_crawl.yml`：

```yaml
name: 每日数据抓取

on:
  schedule:
    - cron: '0 0 * * *'  # 每天UTC 0点（北京时间8点）
  workflow_dispatch:  # 允许手动触发

jobs:
  crawl:
    runs-on: ubuntu-latest

    steps:
      - name: 检出代码
        uses: actions/checkout@v3

      - name: 设置Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: 安装依赖
        run: |
          pip install -r requirements.txt

      - name: 抓取新闻
        env:
          ZHIPU_API_KEY: ${{ secrets.ZHIPU_API_KEY }}
        run: |
          python 数据抓取/新闻爬虫.py
          python AI分析/内容分类.py

      - name: 提交更新
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add 数据/新闻数据.json
          git commit -m "自动更新数据 $(date +'%Y-%m-%d')" || echo "没有变化"
          git push
```

**配置GitHub Secrets：**
1. 进入仓库 Settings → Secrets and variables → Actions
2. 添加 `ZHIPU_API_KEY`

**注意：**
- GitHub Actions每月有2000分钟免费额度
- 每天运行一次，足够使用

### 方案二：本地定时任务

#### Windows系统

创建 `运行爬虫.bat` 文件：

```batch
@echo off
cd /d "C:\路径\汽车行业HR情报监控系统"
python 数据抓取/新闻爬虫.py
python AI分析/内容分类.py
```

使用Windows任务计划程序：
1. 打开"任务计划程序"
2. 创建基本任务
3. 触发器：每天早上8点
4. 操作：启动程序，选择 `运行爬虫.bat`

#### Mac/Linux系统

使用crontab：

```bash
crontab -e
```

添加：
```
0 8 * * * cd /路径/汽车行业HR情报监控系统 && python 数据抓取/新闻爬虫.py && python AI分析/内容分类.py
```

---

## 飞书集成（可选）

### 功能说明
- 每日推送最新HR新闻到飞书群
- 数据同步到飞书多维表格
- 通过飞书机器人查询信息

### 配置步骤

#### 1. 创建飞书机器人

1. 在飞书群中，点击"设置" → "群机器人" → "添加机器人"
2. 选择"自定义机器人"
3. 复制Webhook地址
4. 在 `配置文件.yaml` 中配置：

```yaml
notifications:
  feishu_webhook: "https://open.feishu.cn/open-apis/bot/v2/hook/your-webhook"
  daily_summary: true
  summary_time: "18:00"
```

#### 2. 创建飞书多维表格

1. 在飞书中创建多维表格
2. 添加以下字段：
   - 标题（文本）
   - 公司（单选）
   - HR分类（单选）
   - 摘要（文本）
   - 来源（文本）
   - 链接（URL）
   - 发布时间（日期）

3. 获取表格ID（在URL中）
4. 配置到 `配置文件.yaml`

详细代码实现见 `数据存储/飞书集成.py`（待开发）

---

## 常见问题

### Q1: 爬虫抓不到数据怎么办？

**可能原因：**
1. 网站改版，选择器失效
2. 被反爬虫机制拦截
3. 网络连接问题

**解决方法：**
- 检查网络连接
- 增加 `request_delay` 到3-5秒
- 更新爬虫脚本中的选择器

### Q2: AI分析很慢或失败？

**可能原因：**
- API额度用完
- 网络连接不稳定
- API Key配置错误

**解决方法：**
- 检查智谱AI控制台的额度使用情况
- 验证API Key是否正确
- 减少 `max_news_per_source` 的数量

### Q3: Streamlit界面打不开？

**可能原因：**
- 端口被占用
- 依赖包未安装完整

**解决方法：**
```bash
# 指定其他端口
streamlit run 主应用.py --server.port 8502

# 重新安装依赖
pip install -r requirements.txt --force-reinstall
```

### Q4: 数据不同步怎么办？

**解决方法：**
- 点击界面上的"刷新数据"按钮
- 检查 `数据/新闻数据.json` 文件是否存在
- 重新运行爬虫脚本

### Q5: GitHub Actions无法提交数据？

**可能原因：**
- 没有配置仓库写入权限

**解决方法：**
1. 进入仓库 Settings → Actions → General
2. Workflow permissions 选择 "Read and write permissions"
3. 保存设置

---

## 性能优化建议

### 1. 数据库优化
当新闻数据超过10000条时，建议使用SQLite数据库：

```yaml
storage:
  type: "sqlite"
  sqlite_path: "数据/新闻数据.db"
```

### 2. 缓存优化
在 `主应用.py` 中已配置10分钟缓存：

```python
@st.cache_data(ttl=600)  # 10分钟
def 加载数据():
    ...
```

### 3. 增量抓取
修改爬虫只抓取新数据，避免重复：

```python
# 在爬虫中添加
最后抓取时间 = 获取最后抓取时间()
只抓取晚于(最后抓取时间)
```

---

## 系统维护

### 每周检查项
- [ ] 检查爬虫是否正常运行
- [ ] 查看AI API额度使用情况
- [ ] 验证数据质量

### 每月检查项
- [ ] 清理过期数据（6个月以前）
- [ ] 更新爬虫选择器（如网站改版）
- [ ] 备份数据文件

### 数据备份
```bash
# 备份JSON数据
cp 数据/新闻数据.json 数据/备份_$(date +%Y%m%d).json

# 只保留最近3个月的备份
find 数据/ -name "备份_*.json" -mtime +90 -delete
```

---

## 技术支持

遇到问题时：
1. 查看终端错误信息
2. 检查配置文件格式是否正确
3. 确认API Key是否有效
4. 查看系统日志

---

## 下一步计划

- [ ] 添加更多数据源
- [ ] 支持行业报告PDF解析
- [ ] 添加邮件订阅功能
- [ ] 开发移动端H5页面
- [ ] 集成大模型问答功能

---

## 附录

### 推荐的新闻数据源

| 网站名称 | URL | 特点 |
|---------|-----|------|
| 36氪 | https://36kr.com | 科技创业类新闻 |
| 虎嗅网 | https://www.huxiu.com | 深度商业分析 |
| 钛媒体 | https://www.tmtpost.com | TMT行业资讯 |
| 电动汽车观察家 | https://www.evobserver.com | 新能源汽车专业媒体 |
| 汽车之家 | https://www.autohome.com.cn | 汽车资讯 |
| 晚点LatePost | https://www.latepost.com | 深度报道 |

### 相关技术文档
- Streamlit官方文档: https://docs.streamlit.io/
- 智谱AI API文档: https://open.bigmodel.cn/dev/api
- BeautifulSoup文档: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- GitHub Actions文档: https://docs.github.com/actions

---

**祝你使用愉快！如有问题，欢迎反馈。**
